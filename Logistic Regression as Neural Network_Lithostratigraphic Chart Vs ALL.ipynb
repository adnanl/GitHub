{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression classifier (with Neural Network Mindset) for Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script recognize Lithostratigraphic Chart amoung images from 5 geological iamge classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: dataset_for_lithochart_vs_all_Other_Classes.hdf5 (see the script Data Conversion to HDF5 Format_Lithostratigraphic Chart Vs ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Loading dataset created by the script \"dataset_for_lithochart_vs_all_Other_Classes.hdf5\"\n",
    "    \n",
    "    train_dataset = h5py.File('AllGeoImages/dataset_for_lithochart_vs_all_Other_Classes.hdf5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_img\"][:])      # training Images\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_labels\"][:])   # training Labels\n",
    "\n",
    "    test_dataset = h5py.File('AllGeoImages/dataset_for_lithochart_vs_all_Other_Classes.hdf5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_img\"][:])         # test Images\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_labels\"][:])      # test Labels\n",
    "\n",
    "    classes =np.array([b'Lithostratigraphic Chart',b'Other Geological Image'],dtype='|S25')\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], it's a 'Other Geological Image' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 56\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y_orig[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y_orig[:, index])].decode(\"utf-8\") +  \"' picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 169\n",
      "Number of testing examples: m_test = 57\n",
      "Height/Width of each image: num_px = 224\n",
      "Each image is of size: (224, 224, 3)\n",
      "train_set_x shape: (169, 224, 224, 3)\n",
      "train_set_y shape: (1, 169)\n",
      "test_set_x shape: (57, 224, 224, 3)\n",
      "test_set_y shape: (1, 57)\n"
     ]
    }
   ],
   "source": [
    "# Train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3)\n",
    "    # - m_train (number of training examples)\n",
    "    # - m_test (number of test examples)\n",
    "    # - num_px (= height = width of a training image)\n",
    "    \n",
    "train_set_y=train_set_y_orig\n",
    "test_set_y=test_set_y_orig\n",
    "m_train = train_set_x_orig.shape [0]\n",
    "m_test = test_set_x_orig.shape [0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px ∗ num_px ∗3, 1)\n",
    "\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape (num_px*num_px*3, test_set_x_orig.shape [0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize our dataset\n",
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Architecture of the learning algorithm ##\n",
    "\n",
    "Logistic Regression, using a Neural Network mindset to distinguish Lithostratigraphic Charts from other other geological Images.\n",
    "\n",
    "<img src=\"AllGeoImages/Illustration.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Workflow**:\n",
    "    - Initialize the parameters of the model\n",
    "    - Learn the parameters for the model by minimizing the cost  \n",
    "    - Use the learned parameters to make predictions (on the test set)\n",
    "    - Analyse the results and conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardway Implementation (through Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# FUNCTION 1: Helper function : Sigmoid\n",
    "def sigmoid(z):\n",
    "    \"\"\" Compute the sigmoid of z\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1/(1+ np.exp(-z))\n",
    "    \n",
    "    return s\n",
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n",
    "\n",
    "\n",
    "# FUNCTION 2: initializing parameters\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and Backward propagation\n",
    "\n",
    "\"Forward\" and \"backward\" propagation steps for learning the parameters\n",
    "\n",
    "Workflow:\n",
    "- get X\n",
    "- compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION 3: Propagation\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implementation of a cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    A = sigmoid (np.dot(w.T,X)+b)                               # compute activation\n",
    "    cost = (-1/m)*(np.sum((np.multiply(Y, np.log(A)))+ np.multiply((1-Y),np.log(1-A))))                                 # compute cost\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "   \n",
    "    dw = (1/m)*(np.dot(X,(A-Y).T))\n",
    "    db = (1/m)*np.sum(A-Y)\n",
    "  \n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "Optimization function to update the parameters using gradient descent.\n",
    "The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION 4: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w = w -learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION 5: Predict\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    " \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        \n",
    "        if A[0,i] <= 0.5: \n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all functions into a model\n",
    "Putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
    "\n",
    "Implementation of the model function\n",
    "    - Y_prediction_test for predictions on the test set\n",
    "    - Y_prediction_train for predictions on the train set\n",
    "    - w, costs, grads for the outputs of optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION 6: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test =predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"Train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"Test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_train, \n",
    "         \"Y_prediction_train\" : Y_prediction_test, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 97.0414201183432 %\n",
      "Test accuracy: 77.19298245614036 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'costs': [0.6931471805599453,\n",
       "  0.5009129879393522,\n",
       "  0.490327819625116,\n",
       "  0.4804160317283195,\n",
       "  0.47108509579973573,\n",
       "  0.46226147500163967,\n",
       "  0.4538858939170809,\n",
       "  0.4459099167068676,\n",
       "  0.438293448594686,\n",
       "  0.4310028928944358,\n",
       "  0.4240097770748817,\n",
       "  0.41728971735702775,\n",
       "  0.41082162992020194,\n",
       "  0.40458712347007436,\n",
       "  0.3985700264750379,\n",
       "  0.39275601537091487,\n",
       "  0.3871323192022027,\n",
       "  0.38168748268733954,\n",
       "  0.37641117436730853,\n",
       "  0.3712940298707352,\n",
       "  0.3663275227839147,\n",
       "  0.3615038574150066,\n",
       "  0.35681587907259327,\n",
       "  0.35225699847038733,\n",
       "  0.34782112761460465,\n",
       "  0.34350262509419033,\n",
       "  0.3392962491245329,\n",
       "  0.3351971170264044,\n",
       "  0.33120067007872134,\n",
       "  0.32730264288469546,\n",
       "  0.3234990365491996,\n",
       "  0.31978609509077544,\n",
       "  0.3161602846123139,\n",
       "  0.31261827483508287,\n",
       "  0.3091569226664382,\n",
       "  0.3057732575247091,\n",
       "  0.3024644681885856,\n",
       "  0.29922789097408276,\n",
       "  0.2960609990721217,\n",
       "  0.2929613929041605,\n",
       "  0.28992679137422306,\n",
       "  0.2869550239126306,\n",
       "  0.28404402322140326,\n",
       "  0.2811918186434282,\n",
       "  0.278396530087873,\n",
       "  0.2756563624530596,\n",
       "  0.27296960049551233,\n",
       "  0.2703346041002334,\n",
       "  0.26774980391281133,\n",
       "  0.265213697298525,\n",
       "  0.26272484459782103,\n",
       "  0.26028186565089917,\n",
       "  0.2578834365672884,\n",
       "  0.2555282867188061,\n",
       "  0.25321519593675335,\n",
       "  0.25094299189604247,\n",
       "  0.2487105476708749,\n",
       "  0.24651677944799383,\n",
       "  0.2443606443850018,\n",
       "  0.24224113860241436,\n",
       "  0.24015729529914953,\n",
       "  0.23810818298219794,\n",
       "  0.23609290380191197,\n",
       "  0.23411059198528253,\n",
       "  0.23216041236018087,\n",
       "  0.2302415589639712,\n",
       "  0.2283532537308691,\n",
       "  0.22649474525236551,\n",
       "  0.2246653076059911,\n",
       "  0.22286423924771329,\n",
       "  0.2210908619638479,\n",
       "  0.21934451987860276,\n",
       "  0.21762457851361056,\n",
       "  0.2159304238963476,\n",
       "  0.21426146171409957,\n",
       "  0.21261711651093712,\n",
       "  0.2109968309248803,\n",
       "  0.20940006496291652,\n",
       "  0.20782629531159505,\n",
       "  0.2062750146810944,\n",
       "  0.20474573118078004,\n",
       "  0.2032379677245366,\n",
       "  0.20175126146404493,\n",
       "  0.20028516324851012,\n",
       "  0.19883923710933574,\n",
       "  0.1974130597683843,\n",
       "  0.19600622016852648,\n",
       "  0.19461831902521481,\n",
       "  0.19324896839807193,\n",
       "  0.19189779128133916,\n",
       "  0.19056442121220424,\n",
       "  0.18924850189612727,\n",
       "  0.18794968684823418,\n",
       "  0.18666763904999087,\n",
       "  0.18540203062037877,\n",
       "  0.18415254250082316,\n",
       "  0.1829188641532468,\n",
       "  0.18170069327053598,\n",
       "  0.18049773549883955,\n",
       "  0.17930970417120584],\n",
       " 'Y_prediction_test': array([[1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "         1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         1., 0., 0., 1., 1., 1., 1., 1., 1.]]),\n",
       " 'Y_prediction_train': array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
       " 'w': array([[0.00022805],\n",
       "        [0.00022805],\n",
       "        [0.00022805],\n",
       "        ...,\n",
       "        [0.00022805],\n",
       "        [0.00022805],\n",
       "        [0.00022805]]),\n",
       " 'b': 0.0004578879309186051,\n",
       " 'learning_rate': 0.0001,\n",
       " 'num_iterations': 10000}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 10000, learning_rate = 0.0001, print_cost =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e4e86ea9aeb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot learning curve (with costs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'costs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cost'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iterations (per hundreds)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example of a picture that was wrongly classified.\n",
    "index = 1\n",
    "plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\n",
    "print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[d[\"Y_prediction_test\"][0,index]].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice of learning rate\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print (\"learning rate is: \" + str(i))\n",
    "    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
